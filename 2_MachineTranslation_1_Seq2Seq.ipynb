{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61d3c7ca",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ffb6121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "try:\n",
    "  import tensorflow.keras.backend as K\n",
    "  if len(K.tensorflow_backend._get_available_gpus()) > 0:\n",
    "    from tensorflow.keras.layers import CuDNNLSTM as LSTM\n",
    "    from tensorflow.keras.layers import CuDNNGRU as GRU\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e5f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "BATCH_SIZE = 64  # Batch size for training\n",
    "EPOCHS = 50  # Number of epochs to train for\n",
    "LATENT_DIM = 256  # Latent dimensionality of the encoding space\n",
    "NUM_SAMPLES = 20000  # Number of samples to train on\n",
    "MAX_NUM_WORDS = 40000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20f96a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [] # sentence in original language\n",
    "target_texts = [] # sentence in target language\n",
    "target_texts_inputs = [] # sentence in target language offset by 1 for later teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fabad98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples: 20000\n"
     ]
    }
   ],
   "source": [
    "# load in the data\n",
    "# download the data at: http://www.manythings.org/anki/\n",
    "t = 0\n",
    "for line in open('fra.txt'):\n",
    "  # only keep a limited number of samples\n",
    "  t += 1\n",
    "  if t > NUM_SAMPLES:\n",
    "    break\n",
    "\n",
    "  # input and target are separated by tab\n",
    "  if '\\t' not in line:\n",
    "    continue\n",
    "\n",
    "  # split up the input and translation\n",
    "  input_text, translation, *rest = line.rstrip().split('\\t')\n",
    "\n",
    "  # make the target input and output\n",
    "  # recall we'll be using teacher forcing\n",
    "  target_text = translation + ' <eos>'\n",
    "  target_text_input = '<sos> ' + translation\n",
    "\n",
    "  input_texts.append(input_text)\n",
    "  target_texts.append(target_text)\n",
    "  target_texts_inputs.append(target_text_input)\n",
    "\n",
    "print(\"num samples:\", len(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d35bb4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3342 unique input tokens.\n",
      "Found 9437 unique output tokens.\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "# For translation task, we have 2 languages, so we need 2 tokenizers for inputs and outputs respectively\n",
    "\n",
    "# tokenize the inputs\n",
    "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
    "\n",
    "# get the word to index mapping for input language\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "print('Found %s unique input tokens.' % len(word2idx_inputs))\n",
    "\n",
    "# determine maximum length input sequence\n",
    "max_len_input = max(len(s) for s in input_sequences)\n",
    "\n",
    "\n",
    "# tokenize the outputs\n",
    "# compared to input tokenizer, we specify filters='' bc we don't want to filter out special characters <SOS> and <EOS>\n",
    "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs)\n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)\n",
    "\n",
    "# get the word to index mapping for output language\n",
    "word2idx_outputs = tokenizer_outputs.word_index\n",
    "print('Found %s unique output tokens.' % len(word2idx_outputs))\n",
    "\n",
    "# store number of output words for later\n",
    "# remember to add 1 since indexing starts at 1\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "\n",
    "# determine maximum length output sequence\n",
    "max_len_target = max(len(s) for s in target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f56af42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs.shape: (20000, 5)\n",
      "encoder_inputs[0]: [ 0  0  0  0 17]\n",
      "decoder_inputs[0]: [ 2 47  4  0  0  0  0  0  0  0  0  0]\n",
      "decoder_inputs.shape: (20000, 12)\n"
     ]
    }
   ],
   "source": [
    "# pad the sequences\n",
    "# pad inputs, targets, targets teacher forcing input respectively\n",
    "# padding for inputs uses default as 'pre' and padding for outputs uses padding='post', so that the output of the encoder corresponds to just when you see the last word of the input sentence, and the decoder produces the output imnediately upon seeing the encoder state, rather than having to go through a bunch of zeros first\n",
    "\n",
    "# pad the inputs\n",
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
    "print(\"encoder_inputs.shape:\", encoder_inputs.shape)\n",
    "print(\"encoder_inputs[0]:\", encoder_inputs[0])\n",
    "\n",
    "# pad the teacher forcing inputs for the decoder\n",
    "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n",
    "print(\"decoder_inputs[0]:\", decoder_inputs[0])\n",
    "print(\"decoder_inputs.shape:\", decoder_inputs.shape)\n",
    "\n",
    "# pad the target\n",
    "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23349295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load in pre-trained word vectors\n",
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "f = open(\"glove.6B.100d.txt\", encoding=\"utf-8\")\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1d8b93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "  if i < MAX_NUM_WORDS:\n",
    "    embedding_vector = word2vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      # words not found in embedding index will be all zeros.\n",
    "      embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bd81cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding layer\n",
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=max_len_input,\n",
    "  # trainable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14e87c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot targets, since we cannot use sparse categorical cross entropy when we have sequences\n",
    "decoder_targets_one_hot = np.zeros(\n",
    "  (\n",
    "    len(input_texts),\n",
    "    max_len_target,\n",
    "    num_words_output\n",
    "  ),\n",
    "  dtype='float32'\n",
    ")\n",
    "\n",
    "# assign the values\n",
    "for i, d in enumerate(decoder_targets):\n",
    "  for t, word in enumerate(d):\n",
    "    if word != 0:\n",
    "      decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60563650",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d66b39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "\n",
    "encoder = LSTM(LATENT_DIM, return_state=True)\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "encoder_states = [h, c] # keep only the states to pass into decoder\n",
    "\n",
    "# decoder, using [h, c] as initial state.\n",
    "decoder_inputs_placeholder = Input(shape=(max_len_target,)) # this is the targets input we will use for teacher forcing\n",
    "decoder_embedding = Embedding(num_words_output, EMBEDDING_DIM) # word embedding here will not use pre-trained vectors\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Create the model object\n",
    "model = Model([encoder_inputs_placeholder, decoder_inputs_placeholder], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b9b3472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "250/250 [==============================] - 30s 108ms/step - loss: 5.5023 - acc: 0.2620 - val_loss: 5.2998 - val_acc: 0.2403\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 23s 90ms/step - loss: 4.4651 - acc: 0.3144 - val_loss: 4.9083 - val_acc: 0.3168\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 3.9397 - acc: 0.3975 - val_loss: 4.6159 - val_acc: 0.3656\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 3.5392 - acc: 0.4454 - val_loss: 4.4159 - val_acc: 0.3976\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 3.2023 - acc: 0.4829 - val_loss: 4.2806 - val_acc: 0.4138\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 25s 100ms/step - loss: 2.9168 - acc: 0.5097 - val_loss: 4.1818 - val_acc: 0.4296\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 2.6744 - acc: 0.5309 - val_loss: 4.1186 - val_acc: 0.4369\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 23s 92ms/step - loss: 2.4576 - acc: 0.5502 - val_loss: 4.0785 - val_acc: 0.4462\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 2.2604 - acc: 0.5698 - val_loss: 4.0653 - val_acc: 0.4522\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 28s 112ms/step - loss: 2.0812 - acc: 0.5863 - val_loss: 4.0482 - val_acc: 0.4569\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 1.9160 - acc: 0.6045 - val_loss: 4.0539 - val_acc: 0.4632\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 1.7650 - acc: 0.6224 - val_loss: 4.0595 - val_acc: 0.4657\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 1.6228 - acc: 0.6398 - val_loss: 4.0935 - val_acc: 0.4645\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 1.4926 - acc: 0.6590 - val_loss: 4.0814 - val_acc: 0.4693\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 1.3710 - acc: 0.6775 - val_loss: 4.0839 - val_acc: 0.4735\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 29s 116ms/step - loss: 1.2587 - acc: 0.6953 - val_loss: 4.1020 - val_acc: 0.4750\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 1.1563 - acc: 0.7113 - val_loss: 4.0922 - val_acc: 0.4766\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 26s 102ms/step - loss: 1.0606 - acc: 0.7299 - val_loss: 4.1352 - val_acc: 0.4771\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.9755 - acc: 0.7453 - val_loss: 4.1259 - val_acc: 0.4824\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 28s 114ms/step - loss: 0.8993 - acc: 0.7598 - val_loss: 4.1575 - val_acc: 0.4824\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.8283 - acc: 0.7732 - val_loss: 4.1903 - val_acc: 0.4822\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 25s 100ms/step - loss: 0.7673 - acc: 0.7856 - val_loss: 4.2082 - val_acc: 0.4837\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 25s 99ms/step - loss: 0.7089 - acc: 0.7972 - val_loss: 4.2374 - val_acc: 0.4826\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.6588 - acc: 0.8080 - val_loss: 4.2519 - val_acc: 0.4860\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 24s 98ms/step - loss: 0.6155 - acc: 0.8149 - val_loss: 4.3055 - val_acc: 0.4841\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 24s 95ms/step - loss: 0.5761 - acc: 0.8213 - val_loss: 4.3141 - val_acc: 0.4850\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.5416 - acc: 0.8289 - val_loss: 4.3445 - val_acc: 0.4878\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 23s 94ms/step - loss: 0.5119 - acc: 0.8325 - val_loss: 4.3608 - val_acc: 0.4871\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 25s 100ms/step - loss: 0.4837 - acc: 0.8385 - val_loss: 4.3757 - val_acc: 0.4865\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 28s 112ms/step - loss: 0.4602 - acc: 0.8411 - val_loss: 4.4325 - val_acc: 0.4859\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 25s 100ms/step - loss: 0.4392 - acc: 0.8440 - val_loss: 4.4600 - val_acc: 0.4883\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 0.4199 - acc: 0.8468 - val_loss: 4.4968 - val_acc: 0.4841\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 24s 94ms/step - loss: 0.4042 - acc: 0.8497 - val_loss: 4.5043 - val_acc: 0.4856\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.3895 - acc: 0.8517 - val_loss: 4.5430 - val_acc: 0.4873\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.3773 - acc: 0.8542 - val_loss: 4.5730 - val_acc: 0.4865\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 25s 98ms/step - loss: 0.3648 - acc: 0.8544 - val_loss: 4.5871 - val_acc: 0.4854\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 0.3546 - acc: 0.8566 - val_loss: 4.6173 - val_acc: 0.4870\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 0.3461 - acc: 0.8572 - val_loss: 4.6469 - val_acc: 0.4861\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.3388 - acc: 0.8584 - val_loss: 4.6536 - val_acc: 0.4859\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.3312 - acc: 0.8587 - val_loss: 4.6799 - val_acc: 0.4883\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.3244 - acc: 0.8593 - val_loss: 4.7301 - val_acc: 0.4860\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 25s 100ms/step - loss: 0.3180 - acc: 0.8607 - val_loss: 4.7623 - val_acc: 0.4883\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.3127 - acc: 0.8614 - val_loss: 4.7713 - val_acc: 0.4813\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.3090 - acc: 0.8603 - val_loss: 4.8154 - val_acc: 0.4848\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.3041 - acc: 0.8614 - val_loss: 4.7991 - val_acc: 0.4847\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.3003 - acc: 0.8612 - val_loss: 4.8363 - val_acc: 0.4856\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.2965 - acc: 0.8615 - val_loss: 4.8758 - val_acc: 0.4839\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 0.2940 - acc: 0.8613 - val_loss: 4.9268 - val_acc: 0.4850\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 23s 93ms/step - loss: 0.2897 - acc: 0.8629 - val_loss: 4.9454 - val_acc: 0.4856\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 24s 96ms/step - loss: 0.2882 - acc: 0.8618 - val_loss: 4.9861 - val_acc: 0.4848\n"
     ]
    }
   ],
   "source": [
    "# model training\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "  # both are of shape N x T x K\n",
    "  mask = K.cast(y_true > 0, dtype='float32')\n",
    "  out = mask * y_true * K.log(y_pred)\n",
    "  return -K.sum(out) / K.sum(mask)\n",
    "\n",
    "def acc(y_true, y_pred):\n",
    "  # both are of shape N x T x K\n",
    "  targ = K.argmax(y_true, axis=-1)\n",
    "  pred = K.argmax(y_pred, axis=-1)\n",
    "  correct = K.cast(K.equal(targ, pred), dtype='float32')\n",
    "\n",
    "  # 0 is padding, don't include those\n",
    "  mask = K.cast(K.greater(targ, 0), dtype='float32')\n",
    "  n_correct = K.sum(mask * correct)\n",
    "  n_total = K.sum(mask)\n",
    "  return n_correct / n_total\n",
    "\n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=[acc])\n",
    "\n",
    "r = model.fit(\n",
    "  [encoder_inputs, decoder_inputs], decoder_targets_one_hot,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea1e0552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction model\n",
    "# we need to create another model that can take in the RNN state and previous word as input and accept a T=1 sequence.\n",
    "\n",
    "# encoder\n",
    "# this is to take in the eng sentence, and return the final LSTM h and c\n",
    "encoder_model = Model(encoder_inputs_placeholder, encoder_states)\n",
    "\n",
    "# decoder\n",
    "# using existing layers\n",
    "\n",
    "# initial h and c representations\n",
    "decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
    "decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# input of sequence of 1 since we are only generating one word at a time\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
    "\n",
    "decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
    "decoder_states = [h, c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model([decoder_inputs_single] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "# map indexes back into real words\n",
    "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_trans = {v:k for k, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "667eda9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "  # encode model takes in the input sentence\n",
    "  states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "  # Generate empty target sequence of length 1\n",
    "  # input is 1 x 1, bc 1 sample and 1 time step\n",
    "  target_seq = np.zeros((1, 1))\n",
    "  target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "\n",
    "  # if we get EOS we break\n",
    "  eos = word2idx_outputs['<eos>']\n",
    "\n",
    "  output_sentence = []\n",
    "  for _ in range(max_len_target):\n",
    "    # the decoder model makes prediction\n",
    "    # now we have output probabilities and the new RNN states (h and c)\n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "    # Get next word taking the greedy approach\n",
    "    idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "    # End sentence of EOS\n",
    "    if eos == idx:\n",
    "      break\n",
    "\n",
    "    word = ''\n",
    "    if idx > 0:\n",
    "      word = idx2word_trans[idx]\n",
    "      output_sentence.append(word)\n",
    "\n",
    "    # Update the decoder input, which is just the word just generated\n",
    "    target_seq[0, 0] = idx\n",
    "\n",
    "    # Update states\n",
    "    states_value = [h, c]\n",
    "\n",
    "  return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6a2ec2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random idx:  7291\n",
      "input_seq:  [[   0    0    0   21 1887]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "-\n",
      "Input: Don't despair.\n",
      "Translation: ne dã©sespã©rez pas !\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "i = np.random.choice(len(input_texts))\n",
    "print(\"Random idx: \", i)\n",
    "input_seq = encoder_inputs[i:i+1]\n",
    "print(\"input_seq: \", input_seq)\n",
    "translation = decode_sequence(input_seq)\n",
    "print('-')\n",
    "print('Input:', input_texts[i])\n",
    "print('Translation:', translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a5147d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
