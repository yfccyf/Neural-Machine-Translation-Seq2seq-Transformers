{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde6be4b",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd088f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding, \\\n",
    "  Bidirectional, RepeatVector, Concatenate, Activation, Dot, Lambda\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "try:\n",
    "  import tensorflow.keras.backend as K\n",
    "  if len(K.tensorflow_backend._get_available_gpus()) > 0:\n",
    "    from tensorflow.keras.layers import CuDNNLSTM as LSTM\n",
    "    from tensorflow.keras.layers import CuDNNGRU as GRU\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea4b3bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "LATENT_DIM = 400\n",
    "LATENT_DIM_DECODER = 400 # idea: make it different to ensure things all fit together properly!\n",
    "NUM_SAMPLES = 20000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c29135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [] # sentence in original language\n",
    "target_texts = [] # sentence in target language\n",
    "target_texts_inputs = [] # sentence in target language offset by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9d34927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples: 20000\n"
     ]
    }
   ],
   "source": [
    "# load in the data\n",
    "# download the data at: http://www.manythings.org/anki/\n",
    "t = 0\n",
    "for line in open('fra.txt'):\n",
    "  # only keep a limited number of samples\n",
    "  t += 1\n",
    "  if t > NUM_SAMPLES:\n",
    "    break\n",
    "\n",
    "  # input and target are separated by tab\n",
    "  if '\\t' not in line:\n",
    "    continue\n",
    "\n",
    "  # split up the input and translation\n",
    "  input_text, translation, *rest = line.rstrip().split('\\t')\n",
    "\n",
    "  # make the target input and output\n",
    "  # recall we'll be using teacher forcing\n",
    "  target_text = translation + ' <eos>'\n",
    "  target_text_input = '<sos> ' + translation\n",
    "\n",
    "  input_texts.append(input_text)\n",
    "  target_texts.append(target_text)\n",
    "  target_texts_inputs.append(target_text_input)\n",
    "\n",
    "print(\"num samples:\", len(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb5ac01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3342 unique input tokens.\n",
      "Found 9437 unique output tokens.\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "# tokenize the inputs\n",
    "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
    "\n",
    "# get the word to index mapping for input language\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "print('Found %s unique input tokens.' % len(word2idx_inputs))\n",
    "\n",
    "# determine maximum length input sequence\n",
    "max_len_input = max(len(s) for s in input_sequences)\n",
    "\n",
    "\n",
    "# tokenize the outputs\n",
    "# don't filter out special characters\n",
    "# otherwise <sos> and <eos> won't appear\n",
    "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) # inefficient, oh well\n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)\n",
    "\n",
    "# get the word to index mapping for output language\n",
    "word2idx_outputs = tokenizer_outputs.word_index\n",
    "print('Found %s unique output tokens.' % len(word2idx_outputs))\n",
    "\n",
    "# store number of output words for later\n",
    "# remember to add 1 since indexing starts at 1\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "\n",
    "# determine maximum length output sequence\n",
    "max_len_target = max(len(s) for s in target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63aee2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_data.shape: (20000, 5)\n",
      "encoder_data[0]: [ 0  0  0  0 17]\n",
      "decoder_data[0]: [ 2 47  4  0  0  0  0  0  0  0  0  0]\n",
      "decoder_data.shape: (20000, 12)\n"
     ]
    }
   ],
   "source": [
    "# padding\n",
    "\n",
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
    "print(\"encoder_data.shape:\", encoder_inputs.shape)\n",
    "print(\"encoder_data[0]:\", encoder_inputs[0])\n",
    "\n",
    "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n",
    "print(\"decoder_data[0]:\", decoder_inputs[0])\n",
    "print(\"decoder_data.shape:\", decoder_inputs.shape)\n",
    "\n",
    "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed1beaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load in pre-trained word vectors\n",
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "f = open(\"glove.6B.100d.txt\", encoding=\"utf-8\")\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f0372b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "  if i < MAX_NUM_WORDS:\n",
    "    embedding_vector = word2vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      # words not found in embedding index will be all zeros.\n",
    "      embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "752dec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding layer\n",
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=max_len_input,\n",
    "  # trainable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05d9a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot targets, since we cannot use sparse categorical cross entropy when we have sequences\n",
    "decoder_targets_one_hot = np.zeros(\n",
    "  (\n",
    "    len(input_texts),\n",
    "    max_len_target,\n",
    "    num_words_output\n",
    "  ),\n",
    "  dtype='float32'\n",
    ")\n",
    "\n",
    "# assign the values\n",
    "for i, d in enumerate(decoder_targets):\n",
    "  for t, word in enumerate(d):\n",
    "    if word != 0:\n",
    "      decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa1472",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b73b046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "250/250 [==============================] - 98s 321ms/step - loss: 5.9043 - acc: 0.2529 - val_loss: 5.9236 - val_acc: 0.2223\n",
      "Epoch 2/30\n",
      "250/250 [==============================] - 86s 343ms/step - loss: 5.1999 - acc: 0.2754 - val_loss: 5.5880 - val_acc: 0.2376\n",
      "Epoch 3/30\n",
      "250/250 [==============================] - 85s 339ms/step - loss: 4.6558 - acc: 0.3160 - val_loss: 5.1255 - val_acc: 0.3153\n",
      "Epoch 4/30\n",
      "250/250 [==============================] - 117s 470ms/step - loss: 4.0832 - acc: 0.3788 - val_loss: 4.7654 - val_acc: 0.3611\n",
      "Epoch 5/30\n",
      "250/250 [==============================] - 86s 344ms/step - loss: 3.6074 - acc: 0.4212 - val_loss: 4.5579 - val_acc: 0.3902\n",
      "Epoch 6/30\n",
      "250/250 [==============================] - 83s 333ms/step - loss: 3.2016 - acc: 0.4503 - val_loss: 4.4255 - val_acc: 0.4106\n",
      "Epoch 7/30\n",
      "250/250 [==============================] - 88s 352ms/step - loss: 2.8523 - acc: 0.4740 - val_loss: 4.3492 - val_acc: 0.4140\n",
      "Epoch 8/30\n",
      "250/250 [==============================] - 87s 348ms/step - loss: 2.5430 - acc: 0.4951 - val_loss: 4.3041 - val_acc: 0.4190\n",
      "Epoch 9/30\n",
      "250/250 [==============================] - 83s 333ms/step - loss: 2.2700 - acc: 0.5164 - val_loss: 4.2876 - val_acc: 0.4316\n",
      "Epoch 10/30\n",
      "250/250 [==============================] - 87s 348ms/step - loss: 2.0272 - acc: 0.5400 - val_loss: 4.2864 - val_acc: 0.4341\n",
      "Epoch 11/30\n",
      "250/250 [==============================] - 83s 332ms/step - loss: 1.8122 - acc: 0.5624 - val_loss: 4.2650 - val_acc: 0.4332\n",
      "Epoch 12/30\n",
      "250/250 [==============================] - 80s 320ms/step - loss: 1.6223 - acc: 0.5866 - val_loss: 4.2670 - val_acc: 0.4404\n",
      "Epoch 13/30\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 1.4584 - acc: 0.6096 - val_loss: 4.2945 - val_acc: 0.4459\n",
      "Epoch 14/30\n",
      "250/250 [==============================] - 79s 317ms/step - loss: 1.3180 - acc: 0.6329 - val_loss: 4.2838 - val_acc: 0.4441\n",
      "Epoch 15/30\n",
      "250/250 [==============================] - 86s 346ms/step - loss: 1.1998 - acc: 0.6527 - val_loss: 4.3075 - val_acc: 0.4442\n",
      "Epoch 16/30\n",
      "250/250 [==============================] - 94s 374ms/step - loss: 1.1004 - acc: 0.6687 - val_loss: 4.3482 - val_acc: 0.4441\n",
      "Epoch 17/30\n",
      "250/250 [==============================] - 87s 350ms/step - loss: 1.0177 - acc: 0.6819 - val_loss: 4.3604 - val_acc: 0.4474\n",
      "Epoch 18/30\n",
      "250/250 [==============================] - 88s 352ms/step - loss: 0.9464 - acc: 0.6969 - val_loss: 4.3824 - val_acc: 0.4451\n",
      "Epoch 19/30\n",
      "250/250 [==============================] - 84s 335ms/step - loss: 0.8900 - acc: 0.7043 - val_loss: 4.3923 - val_acc: 0.4438\n",
      "Epoch 20/30\n",
      "250/250 [==============================] - 86s 344ms/step - loss: 0.8389 - acc: 0.7138 - val_loss: 4.4509 - val_acc: 0.4451\n",
      "Epoch 21/30\n",
      "250/250 [==============================] - 88s 351ms/step - loss: 0.7980 - acc: 0.7198 - val_loss: 4.4779 - val_acc: 0.4478\n",
      "Epoch 22/30\n",
      "250/250 [==============================] - 85s 338ms/step - loss: 0.7618 - acc: 0.7268 - val_loss: 4.4790 - val_acc: 0.4488\n",
      "Epoch 23/30\n",
      "250/250 [==============================] - 87s 348ms/step - loss: 0.7316 - acc: 0.7291 - val_loss: 4.5118 - val_acc: 0.4519\n",
      "Epoch 24/30\n",
      "250/250 [==============================] - 87s 349ms/step - loss: 0.7068 - acc: 0.7351 - val_loss: 4.5637 - val_acc: 0.4473\n",
      "Epoch 25/30\n",
      "250/250 [==============================] - 89s 357ms/step - loss: 0.6835 - acc: 0.7372 - val_loss: 4.5864 - val_acc: 0.4495\n",
      "Epoch 26/30\n",
      "250/250 [==============================] - 89s 357ms/step - loss: 0.6642 - acc: 0.7400 - val_loss: 4.5782 - val_acc: 0.4526\n",
      "Epoch 27/30\n",
      "250/250 [==============================] - 86s 342ms/step - loss: 0.6464 - acc: 0.7425 - val_loss: 4.6178 - val_acc: 0.4456\n",
      "Epoch 28/30\n",
      "250/250 [==============================] - 85s 339ms/step - loss: 0.6319 - acc: 0.7439 - val_loss: 4.6507 - val_acc: 0.4519\n",
      "Epoch 29/30\n",
      "250/250 [==============================] - 85s 339ms/step - loss: 0.6161 - acc: 0.7466 - val_loss: 4.6825 - val_acc: 0.4508\n",
      "Epoch 30/30\n",
      "250/250 [==============================] - 86s 343ms/step - loss: 0.6075 - acc: 0.7468 - val_loss: 4.6993 - val_acc: 0.4501\n"
     ]
    }
   ],
   "source": [
    "# training model\n",
    "\n",
    "# the softmax_over_time function\n",
    "def softmax_over_time(x):\n",
    "  assert(K.ndim(x) > 2)\n",
    "  e = K.exp(x - K.max(x, axis=1, keepdims=True))\n",
    "  s = K.sum(e, axis=1, keepdims=True)\n",
    "  return e / s\n",
    "\n",
    "# encoder\n",
    "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = Bidirectional(LSTM(LATENT_DIM, return_sequences=True,))\n",
    "encoder_outputs = encoder(x)\n",
    "\n",
    "# decoder\n",
    "decoder_inputs_placeholder = Input(shape=(max_len_target,))\n",
    "decoder_embedding = Embedding(num_words_output, EMBEDDING_DIM)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "# attention\n",
    "# attention layers need to be global because they will be repeated Ty times at the decoder\n",
    "attn_repeat_layer = RepeatVector(max_len_input)\n",
    "attn_concat_layer = Concatenate(axis=-1)\n",
    "attn_dense1 = Dense(10, activation='tanh')\n",
    "attn_dense2 = Dense(1, activation=softmax_over_time)\n",
    "attn_dot = Dot(axes=1) # to perform the weighted sum of alpha[t] * h[t]\n",
    "\n",
    "def one_step_attention(h, st_1):\n",
    "  # h = h(1), ..., h(Tx), shape = (Tx, LATENT_DIM * 2)\n",
    "  # st_1 = s(t-1), shape = (LATENT_DIM_DECODER,)\n",
    " \n",
    "  # copy s(t-1) Tx times\n",
    "  # now shape = (Tx, LATENT_DIM_DECODER)\n",
    "  st_1 = attn_repeat_layer(st_1)\n",
    "\n",
    "  # Concatenate all h(t)'s with s(t-1)\n",
    "  # Now of shape (Tx, LATENT_DIM_DECODER + LATENT_DIM * 2)\n",
    "  x = attn_concat_layer([h, st_1])\n",
    "\n",
    "  # Neural net first layer\n",
    "  x = attn_dense1(x)\n",
    "\n",
    "  # Neural net second layer with special softmax over time\n",
    "  alphas = attn_dense2(x)\n",
    "\n",
    "  # \"Dot\" the alphas and the h's\n",
    "  # Remember a.dot(b) = sum over a[t] * b[t]\n",
    "  context = attn_dot([alphas, h])\n",
    "\n",
    "  return context\n",
    "\n",
    "# define the rest of the decoder (after attention)\n",
    "decoder_lstm = LSTM(LATENT_DIM_DECODER, return_state=True)\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "\n",
    "initial_s = Input(shape=(LATENT_DIM_DECODER,), name='s0')\n",
    "initial_c = Input(shape=(LATENT_DIM_DECODER,), name='c0')\n",
    "context_last_word_concat_layer = Concatenate(axis=2)\n",
    "\n",
    "# Unlike previous seq2seq, we cannot get the output\n",
    "# all in one step\n",
    "# Instead we need to do Ty steps\n",
    "# And in each of those steps, we need to consider\n",
    "# all Tx h's\n",
    "\n",
    "# s, c will be re-assigned in each iteration of the loop\n",
    "s = initial_s\n",
    "c = initial_c\n",
    "\n",
    "# collect outputs in a list at first\n",
    "outputs = []\n",
    "for t in range(max_len_target): # Ty times\n",
    "\n",
    "  context = one_step_attention(encoder_outputs, s)\n",
    "\n",
    "  # we need a different layer for each time step, which is not global\n",
    "  selector = Lambda(lambda x: x[:, t:t+1])\n",
    "  xt = selector(decoder_inputs_x)\n",
    "  \n",
    "  # combine \n",
    "  decoder_lstm_input = context_last_word_concat_layer([context, xt])\n",
    "\n",
    "  o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s, c])\n",
    "\n",
    "  # final dense layer to get next word prediction\n",
    "  decoder_outputs = decoder_dense(o)\n",
    "  outputs.append(decoder_outputs)\n",
    "\n",
    "def stack_and_transpose(x):\n",
    "    # 'outputs' is now a list of length Ty\n",
    "    # each element is of shape (batch size, output vocab size)\n",
    "    # therefore if we simply stack all the outputs into 1 tensor\n",
    "    # it would be of shape T x N x D\n",
    "    # we would like it to be of shape N x T x D\n",
    "  # x is a list of length T, each element is a batch_size x output_vocab_size tensor\n",
    "  x = K.stack(x) # is now T x batch_size x output_vocab_size tensor\n",
    "  x = K.permute_dimensions(x, pattern=(1, 0, 2)) # is now batch_size x T x output_vocab_size\n",
    "  return x\n",
    "\n",
    "# make it a layer\n",
    "stacker = Lambda(stack_and_transpose)\n",
    "outputs = stacker(outputs)\n",
    "\n",
    "# create the model\n",
    "model = Model(\n",
    "  inputs=[\n",
    "    encoder_inputs_placeholder,\n",
    "    decoder_inputs_placeholder,\n",
    "    initial_s, \n",
    "    initial_c,\n",
    "  ],\n",
    "  outputs=outputs\n",
    ")\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "  # both are of shape N x T x K\n",
    "  mask = K.cast(y_true > 0, dtype='float32')\n",
    "  out = mask * y_true * K.log(y_pred)\n",
    "  return -K.sum(out) / K.sum(mask)\n",
    "\n",
    "\n",
    "def acc(y_true, y_pred):\n",
    "  # both are of shape N x T x K\n",
    "  targ = K.argmax(y_true, axis=-1)\n",
    "  pred = K.argmax(y_pred, axis=-1)\n",
    "  correct = K.cast(K.equal(targ, pred), dtype='float32')\n",
    "\n",
    "  # 0 is padding, don't include those\n",
    "  mask = K.cast(K.greater(targ, 0), dtype='float32')\n",
    "  n_correct = K.sum(mask * correct)\n",
    "  n_total = K.sum(mask)\n",
    "  return n_correct / n_total\n",
    "\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=[acc])\n",
    "\n",
    "# train the model\n",
    "z = np.zeros((len(encoder_inputs), LATENT_DIM_DECODER)) # initial [s, c]\n",
    "r = model.fit(\n",
    "  [encoder_inputs, decoder_inputs, z, z], decoder_targets_one_hot,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb4c9028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction model\n",
    "\n",
    "encoder_model = Model(encoder_inputs_placeholder, encoder_outputs)\n",
    "\n",
    "# next we define a T=1 decoder model\n",
    "encoder_outputs_as_input = Input(shape=(max_len_input, LATENT_DIM * 2,))\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
    "\n",
    "# no need to loop over attention steps this time because there is only one step\n",
    "context = one_step_attention(encoder_outputs_as_input, initial_s)\n",
    "\n",
    "# combine context with last word\n",
    "decoder_lstm_input = context_last_word_concat_layer([context, decoder_inputs_single_x])\n",
    "\n",
    "# lstm and final dense\n",
    "o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\n",
    "decoder_outputs = decoder_dense(o)\n",
    "\n",
    "# note: we don't really need the final stack and tranpose\n",
    "# because there's only 1 output\n",
    "# it is already of size N x D\n",
    "# no need to make it 1 x N x D --> N x 1 x D\n",
    "\n",
    "# create the model object\n",
    "decoder_model = Model(\n",
    "  inputs=[\n",
    "    decoder_inputs_single,\n",
    "    encoder_outputs_as_input,\n",
    "    initial_s, \n",
    "    initial_c\n",
    "  ],\n",
    "  outputs=[decoder_outputs, s, c]\n",
    ")\n",
    "\n",
    "# map indexes back into real words\n",
    "# so we can view the results\n",
    "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_trans = {v:k for k, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27259e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "  # Encode the input as state vectors.\n",
    "  enc_out = encoder_model.predict(input_seq)\n",
    "\n",
    "  # Generate empty target sequence of length 1.\n",
    "  target_seq = np.zeros((1, 1))\n",
    "  \n",
    "  # Populate the first character of target sequence with the start character.\n",
    "  # NOTE: tokenizer lower-cases all words\n",
    "  target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "\n",
    "  # if we get this we break\n",
    "  eos = word2idx_outputs['<eos>']\n",
    "\n",
    "  # [s, c] will be updated in each loop iteration\n",
    "  s = np.zeros((1, LATENT_DIM_DECODER))\n",
    "  c = np.zeros((1, LATENT_DIM_DECODER))\n",
    "\n",
    "\n",
    "  # Create the translation\n",
    "  output_sentence = []\n",
    "  for _ in range(max_len_target):\n",
    "    o, s, c = decoder_model.predict([target_seq, enc_out, s, c])\n",
    "        \n",
    "\n",
    "    # Get next word\n",
    "    idx = np.argmax(o.flatten())\n",
    "\n",
    "    # End sentence of EOS\n",
    "    if eos == idx:\n",
    "      break\n",
    "\n",
    "    word = ''\n",
    "    if idx > 0:\n",
    "      word = idx2word_trans[idx]\n",
    "      output_sentence.append(word)\n",
    "\n",
    "    # Update the decoder input\n",
    "    # which is just the word just generated\n",
    "    target_seq[0, 0] = idx\n",
    "\n",
    "  return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "090d18d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 649ms/step\n",
      "1/1 [==============================] - 1s 514ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "-\n",
      "Input sentence: You tricked us.\n",
      "Predicted translation: tu nous as trompã©s.\n",
      "Actual translation: Tu nous as trompÃ©s. <eos>\n",
      "Continue? [Y/n]y\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "-\n",
      "Input sentence: Let's quit.\n",
      "Predicted translation: renonã§ons.\n",
      "Actual translation: Abandonnons. <eos>\n",
      "Continue? [Y/n]y\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "-\n",
      "Input sentence: He is distracted.\n",
      "Predicted translation: il est paralysã©.\n",
      "Actual translation: Il est dans la lune. <eos>\n",
      "Continue? [Y/n]y\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "-\n",
      "Input sentence: Tom was dirty.\n",
      "Predicted translation: tom ã©tait sale.\n",
      "Actual translation: Tom Ã©tait pourri. <eos>\n",
      "Continue? [Y/n]y\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "-\n",
      "Input sentence: Were they good?\n",
      "Predicted translation: ã‰taient-ils bons ?\n",
      "Actual translation: Ã‰taient-ils bons ? <eos>\n",
      "Continue? [Y/n]n\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "  # Do some test translations\n",
    "  i = np.random.choice(len(input_texts))\n",
    "  input_seq = encoder_inputs[i:i+1]\n",
    "  translation = decode_sequence(input_seq)\n",
    "  print('-')\n",
    "  print('Input sentence:', input_texts[i])\n",
    "  print('Predicted translation:', translation)\n",
    "  print('Actual translation:', target_texts[i])\n",
    "\n",
    "  ans = input(\"Continue? [Y/n]\")\n",
    "  if ans and ans.lower().startswith('n'):\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
